
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/tensorboard_callback.ipynb

#================================================
import torch


#================================================
from IPython.core import debugger as idb


#================================================
import os
import shutil


#================================================
from fastai.basic_train import LearnerCallback


#================================================
from torch.utils.tensorboard import SummaryWriter


#================================================
from fastai.basic_data import DatasetType


#================================================
from collections.abc import Iterable


#================================================
class TensorBoardCallback(LearnerCallback):
    def __init__(self,
                 learn,
                 log_dir,
                 plot_net=False,
                 plot_loss=True,
                 metric_plots=[],
                 hyper_plots=[],
                 hist_plots=[],
                 plot_paramHist=True,
                 plot_gradHist=True,
                 plot_optStateHist=True,
                 hist_iters=10e99 # 这里设置一个很大的数，防止在默认的情况下频繁地绘制hitogram拖慢训练
                ):
        '''
        参数：
        -- learn：一个Learner对象
        -- log_dir：tensorboard writer的写目录
        -- plot_net：True/False，是否绘制网络结构图，一般来说你只在第一次训练的时候绘制网络结构图
        -- plot_loss：是否绘制loss曲线，若为True，则会绘制train loss和smoothed train loss（周期为iteration）
        以及valid loss（周期为epoch）曲线
        -- metric_plots：一个list，用于指定一个或多个metric名称（字符串），被指定了metric会被绘制（周期为epoch）
        -- hyper_plots：一个list，用于指定一个或多个hyper parameter名称（字符串），被指定了的hyper parameter会被绘制（周期为iteration），
        每个param_group的该超参数都会被绘制。
        -- hist_plots：一个list，用于指定一个或多个网络参数的名称（字符串），被指定了的网络参数的直方图（如果plot_paramHist为True）、
        梯度的直方图（如果plot_gradHist为True）、其在optimizer中的各state的直方图（如果plot_optStateHist为True）会被绘制（周期由hist_iters指定）
        -- hist_iters：一个正整数，指定paramHist和gradHist的绘制的周期，单位是iteration
        '''
        self.learn = learn
        self.log_dir = log_dir

        self.plot_net = plot_net

        self.plot_loss = plot_loss

        self.metric_plots = metric_plots # to to: 添加检查
        self.hyper_plots = hyper_plots # to do: 添加检查

        self.hist_plots = hist_plots # to do: 添加检查
        self.plot_paramHist = plot_paramHist
        self.plot_gradHist = plot_gradHist
        self.plot_optStateHist = plot_optStateHist
        self.hist_iters = hist_iters

        # 提取learner的model，方便类内方法访问
        self.model = self.learn.model.module if isinstance(self.learn.model,torch.nn.DataParallel) else self.learn.model
        # 提取模型的所有参数的名称，方便类内方法访问
        self.param_names = [name for (name, _) in self.model.named_parameters()]


    def _clear_logdir(self):
        '清空summarywriter的写目录'
        if os.path.exists(self.log_dir): shutil.rmtree(self.log_dir)
        os.mkdir(self.log_dir)

    def _plot_netGraph(self):
        '绘制网络结构'
        x = self.learn.data.one_batch(DatasetType.Single)[0].to(self.learn.data.device)
        self.writer.add_graph(self.model, x)

    def _plot_trainLoss(self,iteration,**kwargs):
        self.writer.add_scalars('loss',{'train': kwargs['last_loss'],'train_smooth':kwargs['smooth_loss']},iteration)

    def _plot_validLoss(self,iteration,**kwargs):
        self.writer.add_scalars('loss',{'valid': kwargs['last_metrics'][0]},iteration)

    def _plot_metrics(self,iteration,**kwargs):
        for mn in self.metric_plots: # mn: metric name
            idx = self.learn.recorder.metrics_names.index(mn)+1
            self.writer.add_scalar(f'metrics/{mn}',kwargs['last_metrics'][idx],iteration)

    def _plot_hypers(self,iteration):
        for hn in self.hyper_plots: # hn: hyper-parameter name
            for i,pg in enumerate(self.learn.opt.opt.param_groups):
                hp = pg[hn] # hp: hyper-parameter
                if not isinstance(hp,Iterable):
                    self.writer.add_scalar(f'pg{i}/{hn}',hp,iteration)
                else:
                    for j,hpp in enumerate(hp):
                        self.writer.add_scalar(f'pg{i}/{hn}[{j}]',hpp,iteration)

    def _plot_paramHist(self, iteration):
        if len(self.hist_plots)>0 and self.plot_paramHist:
            params = [values.clone().detach().cpu() for (_, values) in self.model.named_parameters()]
            for param_name in self.hist_plots:
                idx = self.param_names.index(param_name)
                param = params[idx]
                self.writer.add_histogram(f'paramHists/{param_name}',param, iteration)

    def _plot_gradHist(self, iteration):
        if len(self.hist_plots)>0 and self.plot_gradHist:
            grads = [param.grad.clone().detach().cpu() for (_, param) in self.model.named_parameters() if param.grad is not None]
            for param_name in self.hist_plots:
                idx = self.param_names.index(param_name)
                grad = grads[idx]
                self.writer.add_histogram(f'gradHists/{param_name}',grad, iteration)

    def _plot_optStateHist(self, iteration):
        if len(self.hist_plots)>0 and self.plot_optStateHist:
            params = [values for (_, values) in self.model.named_parameters()]
            for param_name in self.hist_plots:
                idx = self.param_names.index(param_name)
                param = params[idx]
                state = self.learn.opt.opt.state[param]
                for k,v in state.items():
                    if k!='step':
                        self.writer.add_histogram(f'optStateHists/{param_name}/{k}',v, iteration, bins='fd')


    def on_train_begin(self, **kwargs):
        self._clear_logdir()
        self.writer = SummaryWriter(log_dir=self.log_dir)
        if self.plot_net: self._plot_netGraph()


    def on_backward_begin(self,iteration, **kwargs):
        if self.plot_loss: self._plot_trainLoss(iteration,**kwargs)


    def on_backward_end(self, iteration, **kwargs):
        if iteration%self.hist_iters: return
        self._plot_gradHist(iteration)


    def on_step_end(self, iteration, **kwargs):
        self._plot_hypers(iteration)
        if (iteration+1)%self.hist_iters==0:
            self._plot_optStateHist(iteration)


    def on_batch_end(self, iteration, **kwargs):
        if iteration%self.hist_iters: return
        self._plot_paramHist(iteration)


    def on_epoch_end(self, iteration, **kwargs):
        if self.plot_loss: self._plot_validLoss(iteration,**kwargs)
        self._plot_metrics(iteration,**kwargs)


    def on_train_end(self,**kwargs):
        self.writer.close()

