
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/optimizer.ipynb

#================================================
from torch.optim import Optimizer


#================================================
import torch


#================================================
from functools import partial


#================================================
from collections import defaultdict


#================================================
from IPython.core import debugger as idb


#================================================
class Adam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # update value
                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
from matplotlib import pyplot as plt
import numpy as np
def plot_RAdam_rectifier(beta,length):
    '''
    RAdam中有一个rectifier项，该项仅与梯度平方动量参数beta（在优化器中常称作beta2）有关，且随时间变化。
    该函数提供了rectifier项随时间变化的可视化。
    ------------------------------------
    参数：
    -- beta：梯度平方动量参数beta（在优化器中常称作beta2）
    -- length：绘制前多少个iteration的曲线。
    '''
    rho_inf = 2/(1-beta)-1

    def _cal_r(t):
        rho = rho_inf-(2*t*beta**t)/(1-beta**t)
        if rho>4: r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*(rho_inf))
        else: r = -1
        return rho,r

    rhos = []
    rs = []
    for t in range(1,length):
        rho,r = _cal_r(t)
        rhos += [rho]
        rs += [r]

    _,axs = plt.subplots(1,2,figsize=(10,5))
    axs[0].plot(rs,marker='o')
    axs[1].plot(rhos)
    axs[1].plot(range(length),np.zeros(length)+rho_inf)


#================================================
class RAdam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super(RAdam, self).__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来
        self.rho_infs = [2/(1-group['betas'][1])-1 for group in self.param_groups] # rho:


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group,rho_inf):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t

        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)
        if rho>4:
            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)
        else:
            r = -1

        return bias1,bias2,r


    def _update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group,rho_inf in zip(self.param_groups,self.rho_infs):
            if len(group['params'])==0: continue

            bias1,bias2,r = self._per_group(group,rho_inf)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad
                if group['weight_decay'] != 0:
                    grad = grad.add(p, alpha=group['weight_decay'])

                # state
                state = self.state[p]
                m,v = self._update_state(group,state,grad)

                # debias
                m_hat = m / bias1

                # update value
                if r > 0:
                    v_hat = v / bias2
                    update = group['lr']*m_hat/(v_hat.mul(r).sqrt().add(group['eps']))
                else:
                    update = group['lr']*m_hat
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class SAdam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), smooth=50,
                 weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

        # softplus
        self.sp = torch.nn.Softplus(smooth)

    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # update value
                update = group['lr']*m_hat/(self.sp(v_hat.sqrt()))
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class SAdam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0,
                 smooth=50):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super(SRAdam, self).__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来
        self.rho_infs = [2/(group['betas'][1])-1 for group in self.param_groups]

        # softplus
        self.sp = torch.nn.Softplus(smooth)


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group,rho_inf):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t

        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)
        if rho>4:
            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)
        else:
            r = -1

        return bias1,bias2,r


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group,rho_inf in zip(self.param_groups,self.rho_infs):
            bias1,bias2,r = self._per_group(group,rho_inf)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # dibias
                m_hat = m / bias1

                # update value
                if r > 0:
                    v_hat = v / bias2
                    update = group['lr']*m_hat/self.sp(v_hat.mul(r).sqrt())
                else:
                    update = group['lr']*m_hat
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class LAMB(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # adam step
                adam_step = m_hat/(v_hat.sqrt().add(group['eps']))
                if group['weight_decay'] != 0: adam_step += group['weight_decay']*p.data

                # adam step scale
                adam_step_scale = adam_step.pow(2).mean().sqrt()

                # layer scale
                layer_scale = p.data.pow(2).mean().sqrt()

                # update value
                update = group['lr']*layer_scale*adam_step/adam_step_scale

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class Adam_GCC(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # GCC
                # conv.weight.shape is (ch_out,ch_in,k1,k2)
                if len(list(grad.shape))==4:
                    grad.sub_(grad.mean(dim=[1,2,3], keepdim = True))

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # update value
                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
def conv_not_cinEq256_coutLt256(p):
    'check if p is conv weight, but not (its chin==256 and chout<256).'
    # if not conv weight, return False
    if len(p.shape)<4: return False

    # if cin==256 and cout<256, return False
    if p.shape[1]==256 and p.shape[0]<256: return False

    # else, match condition
    return True

class Stable_Adam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 weight_decay=0, condition_func=conv_not_cinEq256_coutLt256):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        super().__init__(params, defaults)

        self.cond_func = condition_func

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)

                    if self.cond_func(p.data):
                        state_p['init_mag'] = p.data.detach().pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # magnitude change
                if 'init_mag' in state:
                    r = p.data.pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()/state['init_mag']
                else:
                    r = 1.

                # update value
                update = group['lr']*r*m_hat/(v_hat.sqrt().add(group['eps']))
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class Adam_AWD(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
                 condition_func=conv_not_cinEq256_coutLt256):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, eps=eps)
        super().__init__(params, defaults)

        self.cond_func = condition_func

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)

                    if self.cond_func(p.data):
                        state_p['init_mag'] = p.data.detach().pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # update value
                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))

                # do update
                p.sub_(update)

                # adaptive weight decay
                if 'init_mag' in state:
                    p.div_(p.data.pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()).mul_(state['init_mag'])

                # update state with new update-value
                state['update'] = update


#================================================
class SRAdam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0,
                 smooth=50):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super(SRAdam, self).__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来
        self.rho_infs = [2/(group['betas'][1])-1 for group in self.param_groups]

        # softplus
        self.sp = torch.nn.Softplus(smooth)


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group,rho_inf):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t

        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)
        if rho>4:
            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)
        else:
            r = -1

        return bias1,bias2,r


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group,rho_inf in zip(self.param_groups,self.rho_infs):
            if len(group['params'])==0: continue

            bias1,bias2,r = self._per_group(group,rho_inf)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # dibias
                m_hat = m / bias1

                # update value
                if r > 0:
                    v_hat = v / bias2
                    update = group['lr']*m_hat/self.sp(v_hat.mul(r).sqrt())
                else:
                    update = group['lr']*m_hat
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class UM(Optimizer):
    def __init__(self, params, lr=1e-3, beta=0.9, weight_decay=0):
        # create parameter groups
        # 虽然我们仅用到了一个beta，但是fastai的learner要求必须有betas=(#,#)，有点扯，但是...
        defaults = dict(lr=lr, betas=(beta,beta), weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta = group['betas'][0]
        bias = 1-beta**t
        return bias


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta = group['betas'][0]
        state['exp_avg'].mul_(beta).add_(grad, alpha=1-beta)

        return state['exp_avg']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias

                # update value
                update = group['lr']*m_hat/(m_hat.pow(2).mean().sqrt())
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class Pre_LAMB(Optimizer):
    def __init__(self, params, lr=1e-3, beta=0.9, weight_decay=0):
        # create parameter groups
        # 虽然我们仅用到了一个beta，但是fastai的learner要求必须有betas=(#,#)，有点扯，但是...
        defaults = dict(lr=lr, betas=(beta,beta), weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta = group['betas'][0]
        bias = 1-beta**t
        return bias


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta = group['betas'][0]
        state['exp_avg'].mul_(beta).add_(grad, alpha=1-beta)

        return state['exp_avg']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad.data
                grad_hat = grad*p.data.pow(2).mean().sqrt()/grad.pow(2).mean().sqrt()

                # state
                state = self.state[p]
                m = self.update_state(group,state,grad_hat)

                # debias
                m_hat = m / bias

                # update value
                update = group['lr']*m_hat

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class Duv_Adam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()


    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        r = grad/state['exp_avg'] if state['exp_avg']!=0 else 1

        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)
        state['exp_avg_sq'].mul_(beta2).addcmul_(r-1, r-1, value=1-beta2)

        return state['exp_avg'], state['exp_avg_sq']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self.update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # update value
                update = group['lr']*m_hat/(v_hat.sqrt().add(1))
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
class SU_Adam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), weight_decay=0):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg_unit'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_ratio'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**t
        return bias1,bias2


    def _update_state(self,group,state,grad):
        state['step'] += 1

        beta1, beta2 = group['betas']
        s = grad.sign()
        r = torch.where(state['exp_avg_unit']==0, torch.tensor(1.,device=grad.device), s/state['exp_avg_unit'])

        state['exp_avg_unit'].mul_(beta1).add_(s, alpha=1-beta1)
        state['exp_avg_ratio'].mul_(beta2).add_((r-1).abs(), alpha=1-beta2)

        return state['exp_avg_unit'], state['exp_avg_ratio']


    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]
                m,v = self._update_state(group,state,grad)

                # debias
                m_hat = m / bias1
                v_hat = v / bias2

                # layer scale
                layer_scale = p.data.abs().mean()
#                 # rectify abs
#                 ele_abs = ele_abs.clamp_min(ele_abs.mean())

                # update value
                update = group['lr']*layer_scale*m_hat/(1+v_hat)
#                 update = group['lr']*ele_abs*m_hat
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data

                # do update
                p.sub_(update)

                # update state with new update-value
                state['update'] = update


#================================================
def sign_clamp(v,minv):
    res = torch.where(v.abs()>minv,v,v.sign()*minv)
    res = torch.where(res==0,minv,res)
    if res.abs().min()<minv: idb.set_trace()
    return res


#================================================
class B_Adam(Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), weight_decay=0, smooth=50):
        # create parameter groups
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)

        # 初始化state
        self.state = self._get_init_state()

        # softplus
        self.sp = torch.nn.Softplus(smooth)

    def _get_init_state(self):
        state = defaultdict(dict)
        for group in self.param_groups:
            for p in group['params']:
                state_p = state[p]
                if len(state_p) == 0:
                    state_p['step'] = 0
                    state_p['exp_avg_1st'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['exp_avg_2ed'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)
                    state_p['grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        return state

    def _per_group(self,group):
        one_param = group['params'][0] # 获取任一个param
        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1
        beta1, beta2 = group['betas']
        bias1 = 1-beta1**t
        bias2 = 1-beta2**(t-1)
        return bias1,bias2


    @torch.no_grad()
    def step(self):
#         idb.set_trace()
        for group in self.param_groups:
            if len(group['params'])==0: continue

            bias1,bias2 = self._per_group(group)
            beta1,beta2 = group['betas']

            for p in group['params']:
                if p.grad is None:
                    continue

                # grad
                grad = p.grad

                # state
                state = self.state[p]

                # update step
                state['step'] += 1

                # update exp_avg_1st
                m = state['exp_avg_1st'].mul_(beta1).add_(grad, alpha=1-beta1)
                # debias exp_avg_1st
                m_hat = m / bias1

                # layer_scale
                layer_scale = p.data.abs().mean()

                if state['step']==1:
                    update = group['lr']*layer_scale*m_hat
                else:
#                     Phi = (grad-state['grad'])/self.sp(state['update'])
                    Phi = (grad-state['grad'])/sign_clamp(state['update'],state['update'].abs().mean())
                    v = state['exp_avg_2ed'].mul_(beta2).add_(Phi, alpha=1-beta2)
                    v_hat = v/bias2

                    update = group['lr']*layer_scale*m_hat/(self.sp(v_hat.abs()))

                # do update
                if group['weight_decay'] != 0: update += group['weight_decay']*p.data
                p.sub_(update)

                # update state with new update-value
                state['update'] = update
                state['grad'] = grad.data.clone().detach()

