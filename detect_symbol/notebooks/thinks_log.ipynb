{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 模型 resnet18_ssd\n",
    "    - 在3种分辨率上预测，使用单独的ssd head\n",
    "    - 试验了多种优化器，在tiny_ds上训练了5k多个iteration，最优结果的分类正确率不足0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 模型 resnet18_1ssd\n",
    "    - 主要改变在于 **1**ssd，即3个分辨率的预测共用一个 ssd head\n",
    "    - 想法是：\n",
    "        - 每个分辨率单独一个ssd head，则每个 ssd head 只能在相应分辨率的目标上被训练到，训练量就较少。\n",
    "        - 而如果共用一个 ssd head，则 ssd head 的训练量就多了，这有助于 ssd head 的训练\n",
    "        - 但代价时要引入3个 neck block，它们也是要训练的，但我认为它们较简单，应该可以比较容易训练好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 目标检测模型的**数据集内预训练**\n",
    "    - 从craft训练目标检测任务，可能不好训练，因为目标检测涉及到 分类，置信，定位 多个目标\n",
    "    - 提出一种猜想：数据集内预训练\n",
    "        - 以多分类任务预训练\n",
    "        - 按3种分辨率分别预训练\n",
    "    - 生成预训练用的数据集\n",
    "        - 一次仅考虑一个尺度范围内的目标\n",
    "        - 仅保留该类目标，其它尺度的目标被抹掉（涂白或背景填充）\n",
    "    - 预训练\n",
    "        - 每次仅考虑ssd head中于一个尺度范围的目标的**分类相关**的输出的损失函数\n",
    "        - 依次对各个尺度范围单独训练，从小至大训练\n",
    "        - 已经训练好的层保持freeze，不参与后续尺度的训练\n",
    "        - ssd head中分类相关的参数仅参与第一尺度范围的训练，在后续训练中锁定\n",
    "    - 训练\n",
    "        - 预训练完后，以预训练的参数为初始，进行正常的目标检测训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录：\n",
    "    - 数据集：tiny_ds_20200331\n",
    "    - 模型：resnet18_1ssd\n",
    "    - 备注：pConf 和 nConf 使用BCE loss\n",
    "    - 优化器：\n",
    "        - Adam，\n",
    "        - LAMB，\n",
    "        - Pre_LAMB，\n",
    "        - UM\n",
    "    - log：./run_log/20200412/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录：\n",
    "    - 数据集：ds_20200227\n",
    "    - 模型：resnet18_1ssd, partial init to pretrained resnet18\n",
    "    - 损失函数：与“2020-04-12/1.训练记录”一样\n",
    "    - bs：64\n",
    "    - 优化器：\n",
    "        - Adam\n",
    "        - LAMB（发散，中止）\n",
    "    - log：./run_log/20200413\n",
    "    - 简述：提升明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在 fastai 中如何使用 discrimitive lr？\n",
    "    - 在 fastai 中，可以两种方式指定 lr，一个是在 fit 函数中，一个是在 scheduler 中。\n",
    "    - 要使用 discrimitive lr，对这两种方式来说是一样的。\n",
    "    - 首先要设置 learn.layer_groups，要包括多个 layer groups\n",
    "    - 然后再设置 lr 时，将 lr 置为一个 numpy array，其长度与 layer_groups 一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. fastai 的 layer_groups\n",
    "    - fastai 的 learner 的 layer_groups 相当于 pytorch 中的 optimizer 的 param_groups\n",
    "    - 但是当你设置了 learner 的 layer_groups 后，在训练时会发现 optimizer 中 param_groups 的个数是 layer_groups 的 2 倍。\n",
    "    - 这是因为，fastai 把一个 layer_group 内的参数又分成了两部分，第一部分是可以被weight decay的，第二部分是不可以被weight decay的。见 split_no_wd_params 函数，它执行这个划分。\n",
    "    - 什么参数可以被 weight decay，什么参数不可以？\n",
    "        - weight decay 的作用是降低模型的复杂度。\n",
    "        - 模型的参数中，有些是与模型复杂度有关的，例如卷积层和全连接层的weight，这些参数被weight decay是有意义的；\n",
    "        - 有些是与模型的复杂度无关的，例如卷积层和全连接层的bias，batchnorm层的weight和bias，这些参数被weight decay没有意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. fastai 的 optimWraper\n",
    "    - fastai 的 optimWraper 是对 optimizer 的一个包装，它自动执行一些与optimizer相关的常见或者说标准操作\n",
    "    - 它对optimizer的实现做了一些限制，例如你的optimizer的state里必须有betas，且betas必须是一个两元素的tuple\n",
    "    - 它内置的 true weight decay，如果你启用了该功能，则 optimWraper 就会自动执行 weight decay，如果你不清楚这一点，你在optimizer的step中也做了weight decay，那么就相当于执行了两边weight decay，就会有问题。\n",
    "    - 我认为 optimWraper 是一个很不好的设计，它隐形地对你实现 optimizer 时增加了一些限制，甚至挖了一些坑。属于过度服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 对 transfer-learning, finetune 和 discriminative lr 的理解\n",
    "    - transfer learning 是利用已经训练好的模型的backbone，在其基础上添加新的head，适应新任务\n",
    "    - transfer learning 可以对整个模型进行训练，试验过这种方法，效果并不差。\n",
    "    - transfer learning 的另一种方法是，先冻结预训练的backbone，让新添加的部分训练一段时间；然后解冻backbone，对整个模型进行训练。这就是finetune.\n",
    "    - finetune 时，需要用到 discriminative lr: 设置两个 layer_group，一个包含backbone，lr设为0；另一个包含新添加的部分，lr正常设置。在这样的设置下训练一段时间后，等新添加的部分基本收敛了，再将两个group的lr都设为非零，开始训练整个模型。\n",
    "    - discriminative lr 的用途：\n",
    "        - 上面提到，discriminative lr 在 finetune 中很有用，在其它情况下呢？\n",
    "        - 我觉得只应以一种方式使用 discriminative lr：两个group，一个group的lr设为0，另一个的lr设为非零\n",
    "        - 虽然 discriminative lr 的使用可以很灵活，例如分作多个group，且各个group的lr都是不同的非零值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-15**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-4-13，训练记录”之上的变化\n",
    "    - 优化器：Adam\n",
    "    - 设置anneal阈值较高，worseN_thres=10\n",
    "    - 第一次run是先finetune一个stage后全部训练，结果不好\n",
    "    - 第二次run无finetune，但是直到200epoch结束也没有anneal，最终结果与“2020-4-13，训练记录”很接近。（计划：以更长的epoch训练）\n",
    "    - log：./run_log/20200415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 训练计划\n",
    "    - 设置 conf_th = 1\n",
    "    - 使用clas_weights, 令max_imblance=5\n",
    "    - 两种优化器：Stable_Adam和Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To Do (not done):\n",
    "    - Stable_Adam 使用 exp avg 的 ||theta||，而非当前 ||theta||\n",
    "    - 并且其也使用beta1，即与m一致\n",
    "    - 预期这样更改可以使更新步长更稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To Do (not done):\n",
    "    - 自适应weight decay\n",
    "    - 实验发现，使用Stable_Adam优化器时，weight scale增长的更快，这不是一个好现象\n",
    "    - 在文章里(https://blog.janestreet.com/l2-regularization-and-batch-norm/)写道，使用batchnorm而无weight decay时，weight scale会随时间有变大的趋势，而设置合适的weight decay可以抑制这个趋势，使weight scale保持稳定。\n",
    "    - 这个weight decay应该设多大恰好可以抵消这个趋势？不好把握。所以是否可以用“自适应weight decay”\n",
    "    - 自适应wd：\n",
    "        - 应用于后跟bn的卷积层，\n",
    "        - 目标是使其weight scale保持稳定，\n",
    "        - 方法：记录初始scale，在之后的时刻，完成更新后，将weight归一化到初始scale，即乘以 init_scale/current_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-17**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-4-15，1.训练记录”之上的变化:\n",
    "        - conf_th: 0.5 -> 1\n",
    "        - 使用 clas_weights，使 max_imblance=5\n",
    "        - 对比两种优化器：Adam和Stable_Adam，都是全程无freeze\n",
    "        - 设置最大500epoch，保证充分训练\n",
    "    - 第一次run：\n",
    "        - Adam\n",
    "        - 在390个epoch结束\n",
    "        - 最终效果显著提高，valid_loss为1.26\n",
    "    - 第二次run：\n",
    "        - Stable_Adam\n",
    "        - 在38 epoch处手动结束\n",
    "        - 开始时比Adam快，逐渐变慢，在epoch#30开始劣于Adam\n",
    "        - update波动大\n",
    "    - log:\n",
    "        - ./run_log/20200417/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-17, 1.训练记录”之上的变化：\n",
    "        - 对比两种优化器：Adam_AWD和Stable_Adam\n",
    "        - 这两种优化器都是修改了的：由原来对参数按layer的整体scale，改为按chout的整体scale\n",
    "    - 第一次run：\n",
    "        - Adam_AWD\n",
    "        - 在epoch#240左右结束\n",
    "        - 最终效果与“2020-04-17, 1.训练记录，第一次run”基本一致，但是训练时间短了很多\n",
    "        - 观察了conv.weight的整体scale全程较稳定\n",
    "    - 第二次run：\n",
    "        - Stable_Adam\n",
    "        - 因为其它程序抢占cpu，导致该run在epoch#258异常结束\n",
    "        - 直到epoch#258，未anneal，达到valid_loss约1.36\n",
    "        - 从异常结束处继续训练，在epoch#120处结束，最终valid_loss达到了1.16\n",
    "        - 观察了conv.weight的整体scale不断变大，达到了数百甚至上万的量级\n",
    "    - log:\n",
    "        - ./run_log/20200419/\n",
    "        - run_1.csv仅记录了继续训练的部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-19，1.训练记录”之上的变化\n",
    "        - 修改了 resnet_ssd.flatten_grid_anchor 函数，由原来横向（y）先变化改为纵向（x）先变化，以与grid设置一致\n",
    "        - 仅使用Stable_Adam优化器\n",
    "    - run_0：\n",
    "        - Stable_Adam\n",
    "        - 在epoch#240结束\n",
    "        - 进步非常明显，最优valid_loss为0.111，各项metrics几乎理想\n",
    "    - 此次训练所作debug是关键，之前的训练都是有bug的\n",
    "    - 直到训练结束未见valid_loss上升\n",
    "        - 实际上即使在之前有bug的训练中也未见valid_loss上升\n",
    "        - 这说明valid数据与train数据太相似了，实际上valid数据已经不能起到valid的作用了\n",
    "        - 这也说明该符号检测任务的边界非常明确并且范围很小，我们的数据集已经遍历了所有数据分布。\n",
    "        - 这是一种特殊情况\n",
    "    - log：\n",
    "        - ./run_log/20200220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-24**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-20,1.训练记录”之上的变化\n",
    "        - 测试 Adam 和 Adam_AWD 优化器\n",
    "    - run_0:\n",
    "        - Adam\n",
    "        - 在 epoch#165 结束，最好 valid_loss=0.117\n",
    "        - 与 Stable_Adam 相比，结束更快，但 valid_loss 稍差\n",
    "    - run_1：\n",
    "        - Adam_AWD\n",
    "        - 在 epoch#60 处 smooth_loss 出现 inf，导致提前结束\n",
    "        - 未观察到其正常训练结果如何，但可以看出Adam_AWD容易出现loss异常大的值\n",
    "    - log:\n",
    "        - ./run_log/20200224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-26**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-24,1.训练记录”之上的变化\n",
    "        - 测试 Adam_AWD 优化器\n",
    "        - 对fastai库做了修改使smooth_loss不会出现inf\n",
    "    - run_0：\n",
    "        - Adam_AWD\n",
    "        - 在epoch#201结束，最好 valid_loss=0.117\n",
    "        - 与 Adam 结果接近，但结束更慢\n",
    "    - log:\n",
    "        - ./run_log/20200226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-28**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-26，1.训练记录”之上的变化\n",
    "        - 从随机参数开始训练\n",
    "    - run_0:\n",
    "        - Adam\n",
    "        - 在 epoch#295 结束，最优valid_loss=0.102\n",
    "        - 与从预训练的resnet18开始训练相比，更慢，但结果稍好\n",
    "    - run_1：\n",
    "        - Stable_Adam\n",
    "        - 在 epoch#262 结束，最优 valid_loss=0.107\n",
    "        - 与从预训练的resnet18开始训练相比，更慢，但结果稍好\n",
    "        - 与Adam相比，稍快，但结果稍差\n",
    "    - log:\n",
    "        - ./run_log/20200228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020-04-29**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练记录\n",
    "    - 在“2020-04-28，1.训练记录”之上的变化\n",
    "        - 使用class_weights\n",
    "        - lambda_nConf=100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
