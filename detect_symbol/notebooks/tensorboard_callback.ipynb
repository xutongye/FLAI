{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from exp import nb_databunch\n",
    "from exp import nb_resnet_ssd\n",
    "from exp import nb_init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from exp import nb_anchors_loss_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from exp import nb_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.basic_train import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.core import defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from IPython.core import debugger as idb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import LearnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_data import DatasetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## get_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_learn(device=torch.device('cuda'),cache_img=False,opt=nb_optimizer.Adam):\n",
    "    data = nb_databunch.get_databunch(data_root='./data/tiny_ds_20200331',bs=64,device=device,cache=cache_img)  \n",
    "    model = nb_resnet_ssd.get_resnet18_ssd()\n",
    "    model.load_state_dict(torch.load('./models/resnet18_ssd_init.pth'));\n",
    "    learn = Learner(data,model)\n",
    "    if device.type=='cuda':\n",
    "        learn.model = torch.nn.DataParallel(learn.model,device_ids=[0,1,2,3])\n",
    "\n",
    "    # 设置loss function\n",
    "    gvs,_,_,avs,_,_ = nb_anchors_loss_metrics.get_ga()\n",
    "    gaf = nb_anchors_loss_metrics.GridAnchor_Funcs(gvs,avs,device)\n",
    "    learn.loss_func = partial(nb_anchors_loss_metrics.yolo_L, gaf=gaf)\n",
    "    learn.metrics = [partial(nb_anchors_loss_metrics.clas_acc,gaf=gaf),\n",
    "                     partial(nb_anchors_loss_metrics.cent_d,gaf=gaf),\n",
    "                     partial(nb_anchors_loss_metrics.hw_r,gaf=gaf)]\n",
    "\n",
    "    # 设置 optimizer\n",
    "    learn.opt_func = opt\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## TensorboardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class TensorBoardCallback(LearnerCallback):\n",
    "    def __init__(self, \n",
    "                 learn, \n",
    "                 log_dir,\n",
    "                 plot_net=False,\n",
    "                 plot_loss=True,\n",
    "                 metric_plots=[],\n",
    "                 hyper_plots=[],\n",
    "                 hist_plots=[],\n",
    "                 plot_paramHist=True,\n",
    "                 plot_gradHist=True,\n",
    "                 plot_optStateHist=True,\n",
    "                 hist_iters=10e99 # 这里设置一个很大的数，防止在默认的情况下频繁地绘制hitogram拖慢训练\n",
    "                ):\n",
    "        '''\n",
    "        参数：\n",
    "        -- learn：一个Learner对象\n",
    "        -- log_dir：tensorboard writer的写目录\n",
    "        -- plot_net：True/False，是否绘制网络结构图，一般来说你只在第一次训练的时候绘制网络结构图\n",
    "        -- plot_loss：是否绘制loss曲线，若为True，则会绘制train loss和smoothed train loss（周期为iteration）\n",
    "        以及valid loss（周期为epoch）曲线\n",
    "        -- metric_plots：一个list，用于指定一个或多个metric名称（字符串），被指定了metric会被绘制（周期为epoch）\n",
    "        -- hyper_plots：一个list，用于指定一个或多个hyper parameter名称（字符串），被指定了的hyper parameter会被绘制（周期为iteration），\n",
    "        每个param_group的该超参数都会被绘制。\n",
    "        -- hist_plots：一个list，用于指定一个或多个网络参数的名称（字符串），被指定了的网络参数的直方图（如果plot_paramHist为True）、\n",
    "        梯度的直方图（如果plot_gradHist为True）、其在optimizer中的各state的直方图（如果plot_optStateHist为True）会被绘制（周期由hist_iters指定）\n",
    "        -- hist_iters：一个正整数，指定paramHist和gradHist的绘制的周期，单位是iteration\n",
    "        '''\n",
    "        self.learn = learn\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.plot_net = plot_net\n",
    "        \n",
    "        self.plot_loss = plot_loss\n",
    "        \n",
    "        self.metric_plots = metric_plots # to to: 添加检查\n",
    "        self.hyper_plots = hyper_plots # to do: 添加检查\n",
    "        \n",
    "        self.hist_plots = hist_plots # to do: 添加检查\n",
    "        self.plot_paramHist = plot_paramHist\n",
    "        self.plot_gradHist = plot_gradHist\n",
    "        self.plot_optStateHist = plot_optStateHist\n",
    "        self.hist_iters = hist_iters\n",
    "        \n",
    "        # 提取learner的model，方便类内方法访问\n",
    "        self.model = self.learn.model.module if isinstance(self.learn.model,torch.nn.DataParallel) else self.learn.model\n",
    "        # 提取模型的所有参数的名称，方便类内方法访问\n",
    "        self.param_names = [name for (name, _) in self.model.named_parameters()]\n",
    "\n",
    "        \n",
    "    def _clear_logdir(self):\n",
    "        '清空summarywriter的写目录'\n",
    "        if os.path.exists(self.log_dir): shutil.rmtree(self.log_dir)\n",
    "        os.mkdir(self.log_dir)\n",
    "        \n",
    "    def _plot_netGraph(self):\n",
    "        '绘制网络结构'\n",
    "        x = self.learn.data.one_batch(DatasetType.Single)[0].to(self.learn.data.device)\n",
    "        self.writer.add_graph(self.model, x)\n",
    "    \n",
    "    def _plot_trainLoss(self,iteration,**kwargs):\n",
    "        self.writer.add_scalars('loss',{'train': kwargs['last_loss'],'train_smooth':kwargs['smooth_loss']},iteration)\n",
    "        \n",
    "    def _plot_validLoss(self,iteration,**kwargs):\n",
    "        self.writer.add_scalars('loss',{'valid': kwargs['last_metrics'][0]},iteration)\n",
    "            \n",
    "    def _plot_metrics(self,iteration,**kwargs):\n",
    "        for mn in self.metric_plots: # mn: metric name\n",
    "            idx = self.learn.recorder.metrics_names.index(mn)+1\n",
    "            self.writer.add_scalar(f'metrics/{mn}',kwargs['last_metrics'][idx],iteration)\n",
    "            \n",
    "    def _plot_hypers(self,iteration):\n",
    "        for hn in self.hyper_plots: # hn: hyper-parameter name\n",
    "            for i,pg in enumerate(self.learn.opt.opt.param_groups):\n",
    "                hp = pg[hn] # hp: hyper-parameter\n",
    "                if not isinstance(hp,Iterable):\n",
    "                    self.writer.add_scalar(f'pg{i}/{hn}',hp,iteration)\n",
    "                else:\n",
    "                    for j,hpp in enumerate(hp):\n",
    "                        self.writer.add_scalar(f'pg{i}/{hn}[{j}]',hpp,iteration)\n",
    "                        \n",
    "    def _plot_paramHist(self, iteration):\n",
    "        if len(self.hist_plots)>0 and self.plot_paramHist:\n",
    "            params = [values.clone().detach().cpu() for (_, values) in self.model.named_parameters()]\n",
    "            for param_name in self.hist_plots:\n",
    "                idx = self.param_names.index(param_name)\n",
    "                param = params[idx]\n",
    "                self.writer.add_histogram(f'paramHists/{param_name}',param, iteration)\n",
    "                \n",
    "    def _plot_gradHist(self, iteration):\n",
    "        if len(self.hist_plots)>0 and self.plot_gradHist:\n",
    "            grads = [param.grad.clone().detach().cpu() for (_, param) in self.model.named_parameters() if param.grad is not None]\n",
    "            for param_name in self.hist_plots:\n",
    "                idx = self.param_names.index(param_name)\n",
    "                grad = grads[idx]\n",
    "                self.writer.add_histogram(f'gradHists/{param_name}',grad, iteration)\n",
    "                \n",
    "    def _plot_optStateHist(self, iteration):\n",
    "        if len(self.hist_plots)>0 and self.plot_optStateHist:\n",
    "            params = [values for (_, values) in self.model.named_parameters()]\n",
    "            for param_name in self.hist_plots:\n",
    "                idx = self.param_names.index(param_name)\n",
    "                param = params[idx]\n",
    "                state = self.learn.opt.opt.state[param]\n",
    "                for k,v in state.items():\n",
    "                    if k!='step':\n",
    "                        self.writer.add_histogram(f'optStateHists/{param_name}/{k}',v, iteration, bins='fd')\n",
    "                \n",
    "                \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self._clear_logdir()\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        if self.plot_net: self._plot_netGraph() \n",
    "\n",
    "            \n",
    "    def on_backward_begin(self,iteration, **kwargs):\n",
    "        if self.plot_loss: self._plot_trainLoss(iteration,**kwargs)\n",
    "            \n",
    "            \n",
    "    def on_backward_end(self, iteration, **kwargs):\n",
    "        if iteration%self.hist_iters: return\n",
    "        self._plot_gradHist(iteration)\n",
    "    \n",
    "    \n",
    "    def on_step_end(self, iteration, **kwargs):\n",
    "        self._plot_hypers(iteration)\n",
    "        if (iteration+1)%self.hist_iters==0: \n",
    "            self._plot_optStateHist(iteration)\n",
    "        \n",
    "    \n",
    "    def on_batch_end(self, iteration, **kwargs):\n",
    "        if iteration%self.hist_iters: return\n",
    "        self._plot_paramHist(iteration)\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self, iteration, **kwargs):\n",
    "        if self.plot_loss: self._plot_validLoss(iteration,**kwargs)\n",
    "        self._plot_metrics(iteration,**kwargs)\n",
    "            \n",
    "            \n",
    "    def on_train_end(self,**kwargs): \n",
    "        self.writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>clas_acc</th>\n",
       "      <th>cent_d</th>\n",
       "      <th>hw_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.640443</td>\n",
       "      <td>5.043548</td>\n",
       "      <td>0.221999</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>1.231237</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics names:\n",
      "\t\tclas_acc\n",
      "\t\tcent_d\n",
      "\t\thw_r\n",
      "hyper names:\n",
      "\t\tlr\n",
      "\t\tbetas\n",
      "\t\tweight_decay\n",
      "param names:\n",
      "\t\tconv1.weight\n",
      "\t\tbn1.weight\n",
      "\t\tbn1.bias\n",
      "\t\tres_blocks.0.0.conv1.weight\n",
      "\t\tres_blocks.0.0.bn1.weight\n",
      "\t\tres_blocks.0.0.bn1.bias\n",
      "\t\tres_blocks.0.0.conv2.weight\n",
      "\t\tres_blocks.0.0.bn2.weight\n",
      "\t\tres_blocks.0.0.bn2.bias\n",
      "\t\tres_blocks.0.1.conv1.weight\n",
      "\t\tres_blocks.0.1.bn1.weight\n",
      "\t\tres_blocks.0.1.bn1.bias\n",
      "\t\tres_blocks.0.1.conv2.weight\n",
      "\t\tres_blocks.0.1.bn2.weight\n",
      "\t\tres_blocks.0.1.bn2.bias\n",
      "\t\tres_blocks.1.0.conv1.weight\n",
      "\t\tres_blocks.1.0.bn1.weight\n",
      "\t\tres_blocks.1.0.bn1.bias\n",
      "\t\tres_blocks.1.0.conv2.weight\n",
      "\t\tres_blocks.1.0.bn2.weight\n",
      "\t\tres_blocks.1.0.bn2.bias\n",
      "\t\tres_blocks.1.0.downsample.0.weight\n",
      "\t\tres_blocks.1.0.downsample.1.weight\n",
      "\t\tres_blocks.1.0.downsample.1.bias\n",
      "\t\tres_blocks.1.1.conv1.weight\n",
      "\t\tres_blocks.1.1.bn1.weight\n",
      "\t\tres_blocks.1.1.bn1.bias\n",
      "\t\tres_blocks.1.1.conv2.weight\n",
      "\t\tres_blocks.1.1.bn2.weight\n",
      "\t\tres_blocks.1.1.bn2.bias\n",
      "\t\tres_blocks.2.0.conv1.weight\n",
      "\t\tres_blocks.2.0.bn1.weight\n",
      "\t\tres_blocks.2.0.bn1.bias\n",
      "\t\tres_blocks.2.0.conv2.weight\n",
      "\t\tres_blocks.2.0.bn2.weight\n",
      "\t\tres_blocks.2.0.bn2.bias\n",
      "\t\tres_blocks.2.0.downsample.0.weight\n",
      "\t\tres_blocks.2.0.downsample.1.weight\n",
      "\t\tres_blocks.2.0.downsample.1.bias\n",
      "\t\tres_blocks.2.1.conv1.weight\n",
      "\t\tres_blocks.2.1.bn1.weight\n",
      "\t\tres_blocks.2.1.bn1.bias\n",
      "\t\tres_blocks.2.1.conv2.weight\n",
      "\t\tres_blocks.2.1.bn2.weight\n",
      "\t\tres_blocks.2.1.bn2.bias\n",
      "\t\tres_blocks.3.0.conv1.weight\n",
      "\t\tres_blocks.3.0.bn1.weight\n",
      "\t\tres_blocks.3.0.bn1.bias\n",
      "\t\tres_blocks.3.0.conv2.weight\n",
      "\t\tres_blocks.3.0.bn2.weight\n",
      "\t\tres_blocks.3.0.bn2.bias\n",
      "\t\tres_blocks.3.0.downsample.0.weight\n",
      "\t\tres_blocks.3.0.downsample.1.weight\n",
      "\t\tres_blocks.3.0.downsample.1.bias\n",
      "\t\tres_blocks.3.1.conv1.weight\n",
      "\t\tres_blocks.3.1.bn1.weight\n",
      "\t\tres_blocks.3.1.bn1.bias\n",
      "\t\tres_blocks.3.1.conv2.weight\n",
      "\t\tres_blocks.3.1.bn2.weight\n",
      "\t\tres_blocks.3.1.bn2.bias\n",
      "\t\tres_blocks.4.0.conv1.weight\n",
      "\t\tres_blocks.4.0.bn1.weight\n",
      "\t\tres_blocks.4.0.bn1.bias\n",
      "\t\tres_blocks.4.0.conv2.weight\n",
      "\t\tres_blocks.4.0.bn2.weight\n",
      "\t\tres_blocks.4.0.bn2.bias\n",
      "\t\tres_blocks.4.0.downsample.0.weight\n",
      "\t\tres_blocks.4.0.downsample.1.weight\n",
      "\t\tres_blocks.4.0.downsample.1.bias\n",
      "\t\tres_blocks.4.1.conv1.weight\n",
      "\t\tres_blocks.4.1.bn1.weight\n",
      "\t\tres_blocks.4.1.bn1.bias\n",
      "\t\tres_blocks.4.1.conv2.weight\n",
      "\t\tres_blocks.4.1.bn2.weight\n",
      "\t\tres_blocks.4.1.bn2.bias\n",
      "\t\tpred_blocks.0.oconv_loc.weight\n",
      "\t\tpred_blocks.0.oconv_loc.bias\n",
      "\t\tpred_blocks.0.oconv_conf.weight\n",
      "\t\tpred_blocks.0.oconv_conf.bias\n",
      "\t\tpred_blocks.0.oconv_clas.weight\n",
      "\t\tpred_blocks.0.oconv_clas.bias\n",
      "\t\tpred_blocks.0.oconv_hw.weight\n",
      "\t\tpred_blocks.0.oconv_hw.bias\n",
      "\t\tpred_blocks.1.oconv_loc.weight\n",
      "\t\tpred_blocks.1.oconv_loc.bias\n",
      "\t\tpred_blocks.1.oconv_conf.weight\n",
      "\t\tpred_blocks.1.oconv_conf.bias\n",
      "\t\tpred_blocks.1.oconv_clas.weight\n",
      "\t\tpred_blocks.1.oconv_clas.bias\n",
      "\t\tpred_blocks.1.oconv_hw.weight\n",
      "\t\tpred_blocks.1.oconv_hw.bias\n",
      "\t\tpred_blocks.2.oconv_loc.weight\n",
      "\t\tpred_blocks.2.oconv_loc.bias\n",
      "\t\tpred_blocks.2.oconv_conf.weight\n",
      "\t\tpred_blocks.2.oconv_conf.bias\n",
      "\t\tpred_blocks.2.oconv_clas.weight\n",
      "\t\tpred_blocks.2.oconv_clas.bias\n",
      "\t\tpred_blocks.2.oconv_hw.weight\n",
      "\t\tpred_blocks.2.oconv_hw.bias\n"
     ]
    }
   ],
   "source": [
    "# 查看你可以指定哪些metrics，hyper，和parameter\n",
    "# 先快速地进行一次训练，learner才能有recorder\n",
    "learn = get_learn(opt=nb_optimizer.SU_Adam)\n",
    "learn.fit(epochs=1)\n",
    "\n",
    "print('metrics names:')\n",
    "for mt in learn.recorder.metrics_names:\n",
    "    print(f'\\t\\t{mt}')\n",
    "\n",
    "print('hyper names:')\n",
    "for hp in learn.opt.opt_keys:\n",
    "    print(f'\\t\\t{hp}')\n",
    "\n",
    "m = learn.model; \n",
    "m = m.module if isinstance(m,torch.nn.DataParallel) else m;\n",
    "pns = [name for (name, _) in m.named_parameters()]\n",
    "print('param names:')\n",
    "for pn in pns:\n",
    "    print('\\t\\t{}'.format(pn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面cell的代码开始运行后，去命令行启动（或重新启动）tensorboard，就可以在浏览器上看到输出了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 进行3次训练\n",
    "# 指定tensorboard加载的总目录\n",
    "log_root = './tb_log/'\n",
    "# 先清空加载目录\n",
    "if os.path.exists(log_root): shutil.rmtree(log_root)\n",
    "os.mkdir(log_root)\n",
    "# 进行3次训练\n",
    "for i in range(3):\n",
    "    log_dir = f'{log_root}run#{i}'\n",
    "    plot_net = True if i==0 else False # 仅绘制一次网络结构图\n",
    "    \n",
    "    learn = get_learn(opt=nb_optimizer.Adam) # 每次从一个初始化网络开始训练\n",
    "    tbCb = TensorBoardCallback(learn=learn,\n",
    "                               log_dir=log_dir,\n",
    "                               plot_net=plot_net,\n",
    "                               plot_loss=True,\n",
    "                               metric_plots=['cent_d'],\n",
    "                               hyper_plots=['lr'],\n",
    "                               paramHist_plots=['pred_blocks.1.oconv_clas.weight'],\n",
    "                               gradHist_plots=['pred_blocks.1.oconv_clas.weight'],\n",
    "                               optStateHist_plots=['pred_blocks.1.oconv_clas.weight'],\n",
    "                               hist_iters=5)\n",
    "    learn.fit(epochs=3,callbacks=[tbCb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted tensorboard_callback.ipynb to exp/nb_tensorboard_callback.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py --fname 'tensorboard_callback.ipynb' --outputDir './exp/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_tb",
   "language": "python",
   "name": "fastai_tb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
