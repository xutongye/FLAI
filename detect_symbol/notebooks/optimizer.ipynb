{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from IPython.core import debugger as idb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adam ([2017](https://arxiv.org/abs/1412.6980v9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## RAdam ([2019](https://arxiv.org/abs/1908.03265))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&\\rho_{\\infty}\\leftarrow\\frac{2}{1-\\beta_2}-1\\\\\n",
    "&m_0\\leftarrow0\\\\\n",
    "&v_0\\leftarrow0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "\\\\\n",
    "&\\rho_t\\leftarrow\\rho_{\\infty}-\\frac{2t\\cdot\\beta_2^t}{1-\\beta_2^t}\\\\\n",
    "&\\text{if}\\space\\space\\rho_t>4\\space\\space\\text{:}\\\\\n",
    "&\\quad\\quad\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "&\\quad\\quad r_t\\leftarrow\\frac{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2)\\rho_t}{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}\\\\\n",
    "&\\quad\\quad\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{\\sqrt{r_t\\cdot\\hat{v}_t}+\\epsilon}\\\\\n",
    "&\\text{else:}\\\\\n",
    "&\\quad\\quad\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\hat{m}_t\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def plot_RAdam_rectifier(beta,length):\n",
    "    '''\n",
    "    RAdam中有一个rectifier项，该项仅与梯度平方动量参数beta（在优化器中常称作beta2）有关，且随时间变化。\n",
    "    该函数提供了rectifier项随时间变化的可视化。\n",
    "    ------------------------------------\n",
    "    参数：\n",
    "    -- beta：梯度平方动量参数beta（在优化器中常称作beta2）\n",
    "    -- length：绘制前多少个iteration的曲线。\n",
    "    '''\n",
    "    rho_inf = 2/(1-beta)-1\n",
    "    \n",
    "    def _cal_r(t):\n",
    "        rho = rho_inf-(2*t*beta**t)/(1-beta**t)\n",
    "        if rho>4: r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*(rho_inf))\n",
    "        else: r = -1\n",
    "        return rho,r\n",
    "    \n",
    "    rhos = []\n",
    "    rs = []\n",
    "    for t in range(1,length):\n",
    "        rho,r = _cal_r(t)\n",
    "        rhos += [rho]\n",
    "        rs += [r]\n",
    "        \n",
    "    _,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "    axs[0].plot(rs,marker='o')\n",
    "    axs[1].plot(rhos)\n",
    "    axs[1].plot(range(length),np.zeros(length)+rho_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     44
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class RAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "        \n",
    "        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来\n",
    "        self.rho_infs = [2/(1-group['betas'][1])-1 for group in self.param_groups] # rho: \n",
    "        \n",
    "\n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group,rho_inf):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "\n",
    "        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)\n",
    "        if rho>4: \n",
    "            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)\n",
    "        else: \n",
    "            r = -1\n",
    "        \n",
    "        return bias1,bias2,r\n",
    "            \n",
    "        \n",
    "    def _update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group,rho_inf in zip(self.param_groups,self.rho_infs):\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2,r = self._per_group(group,rho_inf)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self._update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "\n",
    "                # update value\n",
    "                if r > 0:\n",
    "                    v_hat = v / bias2\n",
    "                    update = group['lr']*m_hat/(v_hat.mul(r).sqrt().add(group['eps']))\n",
    "                else:\n",
    "                    update = group['lr']*m_hat\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SAdam ([2019](https://arxiv.org/abs/1908.00700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{softplus(\\sqrt{\\hat{v}_t})}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class SAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), smooth=50,\n",
    "                 weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        # softplus\n",
    "        self.sp = torch.nn.Softplus(smooth)\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "            \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(self.sp(v_hat.sqrt()))\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     18,
     30,
     46
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class SAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0,\n",
    "                 smooth=50):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super(SRAdam, self).__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "        \n",
    "        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来\n",
    "        self.rho_infs = [2/(group['betas'][1])-1 for group in self.param_groups]\n",
    "        \n",
    "        # softplus\n",
    "        self.sp = torch.nn.Softplus(smooth)\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group,rho_inf):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        \n",
    "        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)\n",
    "        if rho>4: \n",
    "            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)\n",
    "        else: \n",
    "            r = -1\n",
    "            \n",
    "        return bias1,bias2,r\n",
    "            \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group,rho_inf in zip(self.param_groups,self.rho_infs):\n",
    "            bias1,bias2,r = self._per_group(group,rho_inf)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # dibias\n",
    "                m_hat = m / bias1\n",
    "                \n",
    "                # update value\n",
    "                if r > 0:\n",
    "                    v_hat = v / bias2\n",
    "                    update = group['lr']*m_hat/self.sp(v_hat.mul(r).sqrt())\n",
    "                else:\n",
    "                    update = group['lr']*m_hat\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## LAMB ([2020](https://arxiv.org/abs/1904.00962v5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&A_t\\leftarrow\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}+\\lambda\\cdot\\theta_t\\quad\\quad\\text{# adam step}\\\\\n",
    "\\\\\n",
    "&s_t^l\\leftarrow\\eta\\cdot\\lVert\\theta_t^l\\rVert_2\\cdot\\frac{A_t}{\\lVert A_t\\rVert_2}\\\\\n",
    "&\\theta_{t+1}^l\\leftarrow\\theta_t^l-s_t^l\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LAMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class LAMB(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # adam step\n",
    "                adam_step = m_hat/(v_hat.sqrt().add(group['eps']))\n",
    "                if group['weight_decay'] != 0: adam_step += group['weight_decay']*p.data\n",
    "                \n",
    "                # adam step scale\n",
    "                adam_step_scale = adam_step.pow(2).mean().sqrt()\n",
    "                \n",
    "                # layer scale\n",
    "                layer_scale = p.data.pow(2).mean().sqrt()\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*layer_scale*adam_step/adam_step_scale\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adam_GCC ([2020](https://arxiv.org/abs/2004.01461))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "GCC: Gradint Centeralization for Conv weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&\\text{if }\\theta\\text{ is conv.weight:}\\\\\n",
    "&\\quad\\quad\\tilde{g}_t\\leftarrow g_t-E_{chout}[g_t]\\quad\\quad\\text{# }E_{chout}\\text{ means average on chout}\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot\\tilde{g}_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot\\tilde{g}_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adam_GCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class Adam_GCC(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # GCC\n",
    "                # conv.weight.shape is (ch_out,ch_in,k1,k2)\n",
    "                if len(list(grad.shape))==4: \n",
    "                    grad.sub_(grad.mean(dim=[1,2,3], keepdim = True))\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Stable_Adam (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "想法：\n",
    "- 训练开始时记录各（后跟bn的）层参数的scale，记M0；\n",
    "- 在之后的各次迭代中，以当时的scale（记Mt）与M0的反比来调整step；\n",
    "- stable的含义：原Adam中，后跟bn的卷积层参数的整体尺度会逐渐变大，导致有效学习率反比下降，该方法使有效学习率保持稳定，所以称stable.\n",
    "- 注意：这里求scale并不是对一个weight内的所有元素作为整体求一个scale，而是按chout，一个chout内的元素作为整体求一个scale，总共求chout个scale. \n",
    "- 为什么按chout而不是按layer？对于activation，一个chout是一个特征图，一个特征图内的像素是在不同位置处的特征，但它们是同一个特征；而不同的特征图是不同的特征，不同的特征柔和在一起是没有意义的。在activation中如此，在weight中也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "&\\Theta=\\{\\text{all}\\;\\theta\\;\\text{that is followed by a batchnorm}\\}\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&s_t\\leftarrow\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\\\\\n",
    "\\\\\n",
    "&\\text{if}\\;\\theta\\in\\Theta\\;:\\\\\n",
    "&\\quad\\quad\\text{for}\\;\\text{k}\\;\\text{in}\\;range(chout)\\;:\\\\\n",
    "&\\quad\\quad\\quad\\quad r_t^k\\leftarrow\\frac{\\lVert\\theta_0^k\\rVert_2}{\\lVert\\theta_t^k\\rVert_2}\\\\\n",
    "&\\quad\\quad\\quad\\quad\\theta_{t+1}^k\\leftarrow\\theta_t^k-\\eta\\cdot r_t^k\\cdot s_t\\\\\n",
    "&\\text{else}\\;:\\\\\n",
    "&\\quad\\quad\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot s_t\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Stable_Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def conv_not_cinEq256_coutLt256(p):\n",
    "    'check if p is conv weight, but not (its chin==256 and chout<256).'\n",
    "    # if not conv weight, return False\n",
    "    if len(p.shape)<4: return False\n",
    "    \n",
    "    # if cin==256 and cout<256, return False\n",
    "    if p.shape[1]==256 and p.shape[0]<256: return False\n",
    "    \n",
    "    # else, match condition\n",
    "    return True\n",
    "    \n",
    "class Stable_Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0, condition_func=conv_not_cinEq256_coutLt256):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        self.cond_func = condition_func\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    if self.cond_func(p.data): \n",
    "                        state_p['init_mag'] = p.data.detach().pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # magnitude change\n",
    "                if 'init_mag' in state:\n",
    "                    r = p.data.pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()/state['init_mag']\n",
    "                else: \n",
    "                    r = 1.\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*r*m_hat/(v_hat.sqrt().add(group['eps']))\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Adam_AWD (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adam with Adaptive Weight Decay\n",
    "    - 实验发现，使用Stable_Adam优化器时，weight scale增长的更快，这不是一个好现象\n",
    "    - 在文章里(https://blog.janestreet.com/l2-regularization-and-batch-norm/)写道，使用batchnorm而无weight decay时，weight scale会随时间有变大的趋势，而设置合适的weight decay可以抑制这个趋势，使weight scale保持稳定。\n",
    "    - 这个weight decay应该设多大恰好可以抵消这个趋势？不好把握。所以是否可以用“自适应weight decay”\n",
    "    - 自适应wd：\n",
    "        - 应用于后跟bn的卷积层，\n",
    "        - 目标是使其weight scale保持稳定，\n",
    "        - 方法：记录初始scale，在之后的时刻，完成更新后，将weight归一化到初始scale，即乘以 init_scale/current_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "&\\Theta=\\{\\text{all}\\;\\theta\\;\\text{that is followed by a batchnorm}\\}\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\\\\\n",
    "\\\\\n",
    "&\\text{if}\\;\\theta\\in\\Theta\\;\\text{:}\\\\\n",
    "&\\quad\\quad\\text{for}\\;\\text{k}\\;\\text{in}\\;range(chout)\\;:\\\\\n",
    "&\\quad\\quad\\quad\\quad\\theta_{t+1}^k\\leftarrow\\theta_{t+1}^k\\cdot\\frac{\\lVert\\theta_0^k\\rVert_2}{\\lVert\\theta_{t+1}^k\\rVert_2}\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adam_AWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class Adam_AWD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 condition_func=conv_not_cinEq256_coutLt256):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        self.cond_func = condition_func\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    if self.cond_func(p.data): \n",
    "                        state_p['init_mag'] = p.data.detach().pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(v_hat.sqrt().add(group['eps']))\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # adaptive weight decay\n",
    "                if 'init_mag' in state:\n",
    "                    p.div_(p.data.pow(2).mean(dim=[1,2,3], keepdim=True).sqrt()).mul_(state['init_mag'])\n",
    "                    \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SRAdam (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&\\rho_{\\infty}\\leftarrow\\frac{2}{1-\\beta_2}-1\\\\\n",
    "&m_0\\leftarrow0\\\\\n",
    "&v_0\\leftarrow0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "\\\\\n",
    "&\\rho_t\\leftarrow\\rho_{\\infty}-\\frac{2t\\cdot\\beta_2^t}{1-\\beta_2^t}\\\\\n",
    "&\\text{if}\\space\\space\\rho_t>4\\space\\space\\text{:}\\\\\n",
    "&\\quad\\quad\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "&\\quad\\quad r_t\\leftarrow\\frac{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2)\\rho_t}{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}\\\\\n",
    "&\\quad\\quad\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{softplus\\left(\\sqrt{r_t\\cdot\\hat{v}_t}\\right)}\n",
    "\\quad\\quad\\text{# the only difference from RAdam}\\\\\n",
    "&\\text{else:}\\\\\n",
    "&\\quad\\quad\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\hat{m}_t\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SRAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     18,
     30,
     46
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class SRAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0,\n",
    "                 smooth=50):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super(SRAdam, self).__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "        \n",
    "        # 在RAdam中，betas[1]应该是固定的，所以可以在这里把rho_inf计算出来\n",
    "        self.rho_infs = [2/(group['betas'][1])-1 for group in self.param_groups]\n",
    "        \n",
    "        # softplus\n",
    "        self.sp = torch.nn.Softplus(smooth)\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group,rho_inf):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        \n",
    "        rho = rho_inf - (2*t*beta2**t)/(1-beta2**t)\n",
    "        if rho>4: \n",
    "            r = (rho_inf-4)*(rho_inf-2)*rho/((rho-4)*(rho-2)*rho_inf)\n",
    "        else: \n",
    "            r = -1\n",
    "            \n",
    "        return bias1,bias2,r\n",
    "            \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group,rho_inf in zip(self.param_groups,self.rho_infs):\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2,r = self._per_group(group,rho_inf)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # dibias\n",
    "                m_hat = m / bias1\n",
    "                \n",
    "                # update value\n",
    "                if r > 0:\n",
    "                    v_hat = v / bias2\n",
    "                    update = group['lr']*m_hat/self.sp(v_hat.mul(r).sqrt())\n",
    "                else:\n",
    "                    update = group['lr']*m_hat\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## UM (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "unit momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{\\lVert\\hat{m}_t^{l}\\rVert_2}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### UM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class UM(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta=0.9, weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        # 虽然我们仅用到了一个beta，但是fastai的learner要求必须有betas=(#,#)，有点扯，但是...\n",
    "        defaults = dict(lr=lr, betas=(beta,beta), weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta = group['betas'][0]\n",
    "        bias = 1-beta**t\n",
    "        return bias\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta = group['betas'][0]\n",
    "        state['exp_avg'].mul_(beta).add_(grad, alpha=1-beta)\n",
    "        \n",
    "        return state['exp_avg']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(m_hat.pow(2).mean().sqrt())\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pre_LAMB (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "LAMB解决的问题是：\n",
    "- 由于卷积层后加batchnorm层，使卷积层参数的整体缩放不对网络输出造成影响\n",
    "- 若卷积层参数整体缩放A倍，则梯度尺度变为1/A倍，相对梯度尺度变为1/A^2倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "LAMB解决该问题的思路是：\n",
    "- Adam的更新步长除以自身的整体尺度，解决“梯度尺度变化”的问题\n",
    "- 再乘以该层参数的整体尺度，解决“相对梯度尺度变化”的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adam 本身的 m/v 形式有没有解决“梯度尺度变为1/A倍”的问题？\n",
    "- Adam的更新步长是m/v\n",
    "- 通常v使用的beta很大（>0.99，甚至取到0.999），v对梯度变化的跟随能力很弱，所以当梯度尺度随时间变化时，**v（相对于m）是稳定的**\n",
    "- 相比v来说，m使用的beta较小（0.9），m对梯度变化的跟随能力更强，所以当梯度尺度随时间变化时，**m（相对于v）是变化的**\n",
    "- 所以，如果认为v随时间是相对稳定的，则1/v起到的作用是解决“各层**初始时**梯度尺度不同”的问题，而对解决“梯度尺度随时间变化”没有作用\n",
    "- 所以 m/v 的形式没有解决“梯度尺度变为1/A倍”的问题\n",
    "- 所以，参数尺度变为A倍时，与梯度一样，m/v 也变为 1/A 倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "对Adam的思考：\n",
    "- Adam 中的 m/v 形式的作用，通常理解有两个：\n",
    "    1. 对每个梯度做单位化，使各梯度尺度相当\n",
    "    2. 惩罚梯度噪声，若梯度随时间变化很大，则v会变大，1/v使更新步长变小\n",
    "- 然而，实际上第二各作用是不存在的，因为在大量的实践中，v的beta都使用很大的值（>0.99，甚至0.999)，这使v对梯度变化的跟随能力很弱，v是几乎不随时间变化的。\n",
    "- 对于第一个作用，其实它有两个方面\n",
    "    1. 使各层的梯度尺度相当，这个作用是正面的。\n",
    "    2. 使一层内的各个参数的梯度尺度相当，这个作用是否positive值得考虑，因为这使梯度失去了方向信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pre_LAMB\n",
    "- 直接对 g 除以其自身的整体尺度，即 /||g||_2，解决“梯度尺度变为1/A”的问题\n",
    "- 再对 g 乘以参数的整体尺度，即 \\*||theta||_2，解决“相对梯度尺度变为1/A^2”的问题\n",
    "- 在LAMB中是在Adam step后做自适应scale，而这里是对g做自适应scale，所以命名为pre lamb，意指把自适应scale提前了\n",
    "- 由于对g做了自适应scale，所以m和v都是自适应scale的了，而如果再m/v，这个自适应scale的效果就相除抵消了，\n",
    "- 所以不再做m/v，直接不用v了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "&\\hat{g}_t\\leftarrow\\frac{\\lVert\\theta_t^l\\rVert_2}{\\lVert g_t^l\\rVert_2}\\cdot g_t\\quad\\quad\\text{# pre adaptive scale, compared to initial LAMB}\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot \\hat{g}_t\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "\\\\\n",
    "&s_t\\leftarrow\\eta\\cdot\\hat{m}_t+\\lambda\\cdot\\theta_t\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-s_t\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pre_LAMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class Pre_LAMB(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta=0.9, weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        # 虽然我们仅用到了一个beta，但是fastai的learner要求必须有betas=(#,#)，有点扯，但是...\n",
    "        defaults = dict(lr=lr, betas=(beta,beta), weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta = group['betas'][0]\n",
    "        bias = 1-beta**t\n",
    "        return bias\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta = group['betas'][0]\n",
    "        state['exp_avg'].mul_(beta).add_(grad, alpha=1-beta)\n",
    "        \n",
    "        return state['exp_avg']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # grad\n",
    "                grad = p.grad.data\n",
    "                grad_hat = grad*p.data.pow(2).mean().sqrt()/grad.pow(2).mean().sqrt()\n",
    "                \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m = self.update_state(group,state,grad_hat)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias\n",
    "    \n",
    "                # update value\n",
    "                update = group['lr']*m_hat\n",
    "            \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Duv_Adam (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Debiasd and Unscaled Variance  Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在原Adam（实际上从AdaGrad开始）中，v实际上带偏的，这里把v设计为无偏的，并且v是无标的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&r_t\\leftarrow\\frac{g_t}{m_{t-1}}\\quad\\text{if}\\quad m_{t-1}\\neq0\\quad\\text{else}\\quad1\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot(r_t-1)^2\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\frac{\\hat{m}_t}{1+\\sqrt{\\hat{v}_t}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Duv_Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     21
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class Duv_Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "\n",
    "        \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        r = grad/state['exp_avg'] if state['exp_avg']!=0 else 1\n",
    "        \n",
    "        state['exp_avg'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "        state['exp_avg_sq'].mul_(beta2).addcmul_(r-1, r-1, value=1-beta2)\n",
    "        \n",
    "        return state['exp_avg'], state['exp_avg_sq']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self.update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # update value\n",
    "                update = group['lr']*m_hat/(v_hat.sqrt().add(1))\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Su_ADAM (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scale adaptive and Unit gradient Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**基于几个想法：**\n",
    "- 使用**单位梯度**\n",
    "    - 认为梯度大小跟与最优点的距离之间没有关系，因此仅使用其方向\n",
    "    - 不受小噪声的影响，只要噪声不改变梯度方向，则其不会改变单位梯度\n",
    "- 使用动量\n",
    "    - 单位梯度的动量\n",
    "    - 偶尔的方向变化会使动量幅度下降\n",
    "    - 频繁的方向变化会使动量幅度接近0，频繁的方向变化可能表示两种情况\n",
    "        - 大量的大幅度的噪声\n",
    "        - 在极小点左右震荡\n",
    "- 除以方差动量\n",
    "    - 单位动量符号变化约频繁，则方差动量越大，则更新步长越小\n",
    "    - 方差动量与单位梯度动量的作用一致，加强了“方向变化时减小更新步长”的效果\n",
    "- 更新步长**正比于参数模**\n",
    "    - 参数模越大则更新幅度越大，成正比\n",
    "    - 使用单个参数本身的模还是一批（比例同一个channel）参数的整体的模？\n",
    "        - 使用一批参数的模的问题：不能体现出这一批内部的各个参数的大小的差异，尤其是可能有模较大的参数，这些参数的更新速度会被拖慢。\n",
    "        - 使用单个参数的模的问题：参数非常小时更新非常慢，尤其是参数模为0时停止更新\n",
    "        - 方法：以一批参数的模的平均值为下限，单个参数的模低于下限时使用该下限，单个参数的模高于下限时使用单个参数的模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_0=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "&s_t\\leftarrow\\text{sign}(g_t)\\\\\n",
    "\\\\\n",
    "&r_t\\leftarrow\\frac{s_t}{m_{t-1}}\\quad\\text{if}\\quad m_{t-1}\\neq0\\quad\\text{else}\\quad1\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot s_t\\\\\n",
    "&v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot\\lvert r_t-1\\rvert\\\\\n",
    "\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "&\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^t}\\\\\n",
    "\\\\\n",
    "&\\epsilon_t\\leftarrow E[\\lvert\\theta_{t}\\rvert^{layer}]\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-\\eta\\cdot\\epsilon_t\\cdot\\frac{\\hat{m}_t}{1+\\hat{v}_t}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SU_Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class SU_Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), weight_decay=0):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "    \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg_unit'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_ratio'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**t\n",
    "        return bias1,bias2\n",
    "        \n",
    "        \n",
    "    def _update_state(self,group,state,grad):\n",
    "        state['step'] += 1\n",
    "        \n",
    "        beta1, beta2 = group['betas']\n",
    "        s = grad.sign()\n",
    "        r = torch.where(state['exp_avg_unit']==0, torch.tensor(1.,device=grad.device), s/state['exp_avg_unit'])\n",
    "        \n",
    "        state['exp_avg_unit'].mul_(beta1).add_(s, alpha=1-beta1)\n",
    "        state['exp_avg_ratio'].mul_(beta2).add_((r-1).abs(), alpha=1-beta2)\n",
    "        \n",
    "        return state['exp_avg_unit'], state['exp_avg_ratio']\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                m,v = self._update_state(group,state,grad)\n",
    "                \n",
    "                # debias\n",
    "                m_hat = m / bias1\n",
    "                v_hat = v / bias2\n",
    "                \n",
    "                # layer scale\n",
    "                layer_scale = p.data.abs().mean()\n",
    "#                 # rectify abs\n",
    "#                 ele_abs = ele_abs.clamp_min(ele_abs.mean())\n",
    "                              \n",
    "                # update value\n",
    "                update = group['lr']*layer_scale*m_hat/(1+v_hat)\n",
    "#                 update = group['lr']*ele_abs*m_hat\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                \n",
    "                # do update\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## B_ADAM (xty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Binomial Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\text{initial:}&\\\\\n",
    "&m_0=0\\\\\n",
    "&v_1=0\\\\\n",
    "\\\\\n",
    "\\text{step:}&\\\\\n",
    "&g_t\\leftarrow\\bigtriangledown_{\\theta}\\mathcal{L}(S_t,\\theta_t)\\\\\n",
    "\\\\\n",
    "&m_t\\leftarrow\\beta_1\\cdot m_{t-1}+(1-\\beta_1)\\cdot g_t\\\\\n",
    "&\\hat{m}_t\\leftarrow\\frac{m_t}{1-\\beta_1^t}\\\\\n",
    "\\\\\n",
    "&\\epsilon_t\\leftarrow E[\\lvert\\theta_{t}\\rvert^{layer}]\\\\\n",
    "\\\\\n",
    "&\\text{if}\\quad t>1:\\\\\n",
    "&\\quad\\quad\\Phi_t\\leftarrow\\frac{g_t-g_{t-1}}{\\text{softplus}(s_{t-1})}\\\\\n",
    "&\\quad\\quad v_t\\leftarrow\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot\\Phi_t\\\\\n",
    "&\\quad\\quad\\hat{v}_t\\leftarrow\\frac{v_t}{1-\\beta_2^{t-1}}\\\\\n",
    "\\\\\n",
    "&\\quad\\quad s_t\\leftarrow\\eta\\cdot\\epsilon_t\\cdot\\frac{\\hat{m}_t}{\\lvert\\hat{v}_{t-1}\\rvert}\\\\\n",
    "&\\text{else :}\\\\\n",
    "&\\quad\\quad s_t\\leftarrow\\eta\\cdot\\epsilon_t\\cdot\\hat{m}_t\\\\\n",
    "\\\\\n",
    "&\\theta_{t+1}\\leftarrow\\theta_t-s_t\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### B_Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def sign_clamp(v,minv):\n",
    "    res = torch.where(v.abs()>minv,v,v.sign()*minv)\n",
    "    res = torch.where(res==0,minv,res)\n",
    "    if res.abs().min()<minv: idb.set_trace()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class B_Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), weight_decay=0, smooth=50):            \n",
    "        # create parameter groups\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "        # 初始化state\n",
    "        self.state = self._get_init_state()\n",
    "        \n",
    "        # softplus\n",
    "        self.sp = torch.nn.Softplus(smooth)\n",
    "    \n",
    "    def _get_init_state(self):\n",
    "        state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state_p = state[p]\n",
    "                if len(state_p) == 0:\n",
    "                    state_p['step'] = 0\n",
    "                    state_p['exp_avg_1st'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['exp_avg_2ed'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state_p['grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        return state\n",
    "        \n",
    "    def _per_group(self,group):\n",
    "        one_param = group['params'][0] # 获取任一个param\n",
    "        t = self.state[one_param]['step']+1 # 此时step是未更新的，所以+1\n",
    "        beta1, beta2 = group['betas']\n",
    "        bias1 = 1-beta1**t\n",
    "        bias2 = 1-beta2**(t-1)\n",
    "        return bias1,bias2\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "#         idb.set_trace()\n",
    "        for group in self.param_groups:\n",
    "            if len(group['params'])==0: continue\n",
    "                \n",
    "            bias1,bias2 = self._per_group(group)\n",
    "            beta1,beta2 = group['betas']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                # grad\n",
    "                grad = p.grad\n",
    "                    \n",
    "                # state\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # update step\n",
    "                state['step'] += 1\n",
    "                \n",
    "                # update exp_avg_1st\n",
    "                m = state['exp_avg_1st'].mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "                # debias exp_avg_1st\n",
    "                m_hat = m / bias1\n",
    "                \n",
    "                # layer_scale\n",
    "                layer_scale = p.data.abs().mean()\n",
    "                \n",
    "                if state['step']==1:\n",
    "                    update = group['lr']*layer_scale*m_hat\n",
    "                else:\n",
    "#                     Phi = (grad-state['grad'])/self.sp(state['update'])\n",
    "                    Phi = (grad-state['grad'])/sign_clamp(state['update'],state['update'].abs().mean())\n",
    "                    v = state['exp_avg_2ed'].mul_(beta2).add_(Phi, alpha=1-beta2)\n",
    "                    v_hat = v/bias2\n",
    "                    \n",
    "                    update = group['lr']*layer_scale*m_hat/(self.sp(v_hat.abs()))\n",
    "                    \n",
    "                # do update\n",
    "                if group['weight_decay'] != 0: update += group['weight_decay']*p.data\n",
    "                p.sub_(update)\n",
    "                \n",
    "                # update state with new update-value\n",
    "                state['update'] = update\n",
    "                state['grad'] = grad.data.clone().detach()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted optimizer.ipynb to ../exp/optimizer.py\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../notebook2script.py --fname 'optimizer.ipynb' --outputDir '../exp/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
